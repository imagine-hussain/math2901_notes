\section{Estimators}

\subsection{Data and Models}

\paragraph{Samples and Data}
A random sample is a collection of (random) observations, namely
\((X_1, \ldots, X_n)\).
The sample data is \(x_1, \ldots, x_n\).
For the sake of laziness, this may be represented as
\(X[1, n]\) and  \(x[1, n]\) respectively.
This is not standard notation.

\paragraph{Parametric Model}
A parametric model for a random sample \(X[1, n]\)
is a family of probability density functions
\(f(x : \theta)\) for \(\theta in \Theta\) where \(\Theta \in  \mathbb{R}^d\)
is called the parameter space.

Data being modelled by a parametric family \(f(x; \theta): \theta \in  Theta\)
can be written as \[
  X[1, n] \sim \{f(x; \theta) : \theta \in \Theta\}, \text{ for } x \in \mathbb{R}
.\] 

\subsection{Estimators}

\paragraph{Estimators}
Suppose that \(X[1, n] \sim \{f(x; \theta) : \theta \in \Theta\} \).
As estimator of \(\theta\) is denoted by \((\hat{\theta)}_n\).
That is, \[
  \hat{\theta}_n = \hat{\theta}_n(X[1, n]) = g(X[1, n]),
\]  where \(g: \mathbb{R}^n \to mathbb{R}\).

\paragraph{Estimators of Mean of Normal Distribution}
Suppose that \(X[1, n]\) are independent samples from a normal distribution
\(\Normal(\mu, \sigma^2)\).

Then, there are a few methods of calculating the mean:
\begin{itemize}
  \item Average: \(\hat{\mu}_n = \frac{1}{n} \sum_{i=1}^n X_i \)
  \item Median: \(\hat{\mu}_n = X_{n/2}\)
  \item  \(\hat{\mu}_n = \frac{X_1 + X_n}{2}\).
\end{itemize}
By the law of large numbers, the first estimator approaches \(\mu\) and
is the best estimator.

\paragraph{Bias}
The bias is a way to assess how good an estimator is.
Is is calculated as \[
  \Bias(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta
.\] 
If  \(\Bias(\hat{\theta}) = 0\) then, \(\theta\) is an unbiased estimator.

\paragraph{Variance Estimators}
From the definition of variance, we may infer that if \(\hat{\mu}_n = \overline{X}_n\) \[
  \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \hat{\mu})^2
.\] 
As such \(\Bias(\hat{\sigma}^2 = \mathbb{E}(\hat{sigma}^2) - \sigma^2)\). Equivalently, \[
  \Bias(\hat{\sigma}^2) = \sigma^2 \left( 1-\frac{1}{n} \right)  - \sigma^2
  = sigma^2\left(  - \frac{1}{n} \right) 
.\] 
On average, \(\hat{sigma}^2\) will underestimate the variance but,
the becomes unbiased for large \(n\).

\paragraph{Student \(t\)-Distribution}
A random variable \(T\) has a \(t\)-distribution with a degree
of freedom  \(v\) if it has a probability density function of \[
  f_T(x) = \frac{\Gamma(\nu/2)}{\Gamma(\nu/2)\Gamma(1/2)}
  v^{-\frac{1}{2}} \left( 1 + \frac{x^2}{\nu} \right)^{(1-v)/2}
.\] 
This looks like a normal distribution. In fact, as \(\nu \to  \infty\),
\(T_\nu \to Z \sim \Norm(0, 1)\). The \(t\)-distribution has
a fatter tail.

\paragraph{Normal and \(t\)-Distribution}
Let \(Y, Z\) be independent random variables with
\(Y \sim \Normal(0, 1)\) and \(Z \sim \chi_\nu^2\).

Recall that \(\chi^2_n\) is equivalent to the distribution \(\Gamma(n, \frac{1}{2})\).

\subsection{Errors}

\paragraph{Standard Error}
The standard error of a estimator \(\hat{\sigma}\) is \[
  \Se(\hat{\theta}) = \frac{1}{\sqrt{n} } \sqrt{\Var(\hat{\theta})}
.\] 
Similarly, the estimated standard error is \[
  \hat{\Se(\hat{\theta})} = \frac{1}{\sqrt{n} }
  \left[ \sqrt{\Var(\hat{\theta})} \right]_{\theta = \hat{\theta}}
.\] 

\paragraph{Mean Square Error}
Define Mean Square Error (MSE) of an estimator to be \[
  \MSE(\hat{\theta}) \coloneqq \mathbb{E}\left( \left( \hat{\theta}  - \hat{\theta}\right)^2 \right) 
.\] 
Equivalently, \[
  \MSE(\hat{\theta}) = \Var(\hat{\theta}) + [\Bias(\hat{\theta})]^2
.\] 

This is estimated as \[
  \hat{\MSE(\hat{\theta})} = \Bias(\hat{\theta})^2 + \Se(\hat{\theta})
.\] 

\paragraph{Variance Bias Tradeoff}
For a fixed \(\MSE\), there is a trade-off between variance and bias.
In statistics, we care for reducing bias most of the time.
However, in other applications such as machine learning, there is a
need for low variance.

\paragraph{Comparing Estimators with MSE}
Suppose that \(\hat{\theta_1}, \hat{\theta_2}\) are two estimators of \(\theta\).
Then, \(\hat{\theta_1}\) is better than \(\hat{\theta_2}\) at \(\theta_0\)
if \[
  \MSE_{\theta_0}(\hat{\theta_1}) < \MSE(\hat{\theta_2})
.\]

A estimator \(\hat{\theta_1}\) is uniformly better than \(\hat{\theta_2}\) if,
\(\hat{\theta_1}\) is better for all  \(\theta \in  \Theta\).

\paragraph{Asymptotic Properties of the Estimator}
An estimator \(\hat{\theta}\) is a consistent estimator of \(\theta\) if as
\(n \to  \infty\), \[
  \hat{\theta} \xrightarrow{\mathbb{P}} \theta
.\] 
By the weak law of large numbers \(\overline{X}\) is a consistent estimator of \(\mu\).
We say that the estimators are \textit{asymptoticlly} unbiased.

% TODO MOVE DISTS FOR NORMAL TO OWN SECTION AND FINISh
\paragraph{TODO:} MOVE DISTS FOR NORMAL TO OWN SECTION AND FINISH

\paragraph{Distribution Related to Normal Distribution}
Given a random sample \(X[1, n]\) that follows  \(\Normal(\mu, \sigma^2)\)
where all \(X_i \sim \Normal(\mu_i, \sigma_i^2\) with independent \(X_i\).

\paragraph{Consistency}
Consistency of an estimator is its performance as the amount of data increases.
For reasonable estimators, we expect that \(\theta = \hat{\theta}\) improves for large \(n\).
We define an estimator \(\hat{\theta}_n\) to be consistent for \(\theta\)
if \[
  \hat{\theta}_n \xrightarrow{\mathbb{P}}.
.\] 

In cases where checking convergence of probability is difficult, we may equivalently test
that \[
  \lim_{n \to \infty} \MSE(\hat{\theta}_n) = 0
.\] 

\paragraph{Asymptomatic Normality}
The estimator is asymptotically normal if  \[
  \frac{\hat{\theta} - \theta}{\Se(\hat{\theta})} \xrightarrow{d} \Norm(0,1).
.\] 
We know from the central limit theorem that \(\hat{\mu} = \overline{X}\) and,
a sample proportion \(\hat{p}\) are asymptotically normal.

\subsection{Notation and Common Practice}

\paragraph{Good Notation For Standard Error and Observed Values}
It is common to add the standard error in parenthesis after reporting an observed value.
That is, if \(x_i = 100\) with a standard error  \(10\), then we may state \[
  x_i = 100(10)
.\] 

\paragraph{Tau Transformation for Gamma Models}
For a \(\Gamma(\alpha, \beta)\) distribution, we may often want
to use the transformation  \(\tau = \alpha \beta\). This corresponds
to the mean of the distribution.


\subsection{Confidence Intervals}
The value of an estimator is not sufficient to let us know of the inherent variability
of that estimator. Confidence intervals ameliorate this by proving a \textit{range}
of values.

Suppose that \(X[1, n]\) is a random sample from a model with a know parameter  \(\theta\).
Let \[
  L = L(X[1, n]) \quad \text{ and } \quad U = U(X[1, n])
\]  be statistics (functions) for \(X_i\)'s such that  \[
  \mathbb{P}(L < \theta < U) \geq 1 - \alpha, \quad \text{for all } \theta > \Theta
.\] 

Then, \((L, U)\) is a  \(1 - \alpha\) or,  \(100(1 - \alpha)\%\) confidence interval.

\paragraph{General Steps to Find Confidence Interval}
\begin{enumerate}
  \item Find an estimator \(\hat{\theta}\) of \(\theta\).
  \item Find some invertible function \(g\) linking  \(\theta\) and \(\hat{\theta}\) for which,
    you know the distribution.
  \item To obtain a (symmetric) \(100(1 - \alpha)\) percent interval, notice  \[
      \mathbb{P}(v_{\frac{\alpha}{2}} < g(\theta, \hat{\theta}) \leq v_{1 - \frac{\alpha}{2}}) = 1 - \alpha
    ,\]  where \(v_c\) is the  \(100c\)-th percent of the distribution \(g(\hat{\theta}, \theta)\).
  \item Invert \(g\) to obtain a confidence interval for  \(\theta\).
\end{enumerate}

\paragraph{Confidence Interval For a Normal Sample}
Recall that for \(X[1, n] \sim \Norm(\mu, \sigma)\), we derive that \[
  \frac{\overline{X} - \mu}{S/\sqrt{n}} \sim t_{n - 1}
  \quad \text{ and } \quad
  \frac{(n - 1)S^2}{\sigma^2} \sim \chi^2_{n - 1}
\]  allow for exact confidence intervals for \(\mu, \sigma\).

A \(1 - \alpha\) confidence interval for  \(\mu\) is  \[
  \left(
    \overline{X} - T_{n - 1, 1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n} },
    X + t_{n - 1, 1 - \frac{\alpha}{2}} \frac{S}{\sqrt{n}}
  \right)
.\]

\paragraph{Confidence For Comparing Random Normal Samples}
Suppose that \[
  X[1, n] \sim \Norm(\mu_X, \sigma_X^2), 
  Y[1, n] \sim \Norm(\mu_Y, \sigma_Y^2)
.\] 

Then, to compare their ostensible means, we take a confidence interval for
\(\mu_X - \mu_Y\) is \[
  \hat{X} - \hat{Y} \pm t_{n_X = N_y - 2, 1 - \frac{a}{2}} S_P \sqrt{\frac{1}{n_x} + \frac{1}{n_Y}} 
.\] 
Here, we may call the following the \textit{pooled sample variance} \[
  S_p^2 = \frac{(n_X - 1)S^2_X + (n_Y - 1S^2_Y}{n_X + n_Y - 1}
.\] 

\paragraph{Confidence Interval for Paired Normal Random Sample}
Suppose that \((X, Y)\) is a paired normal random sample. We can construct the confidence interval
for the mean difference  \(D = X - Y\).

