\section{Bivariate Distributions}

\paragraph{Definition: Bivariate Distribution}
The joint density function of two continuous random variables \(X, Y\) is
given by a bivariate functiosn \(f_{x, y}\) with the following properties
\begin{enumerate}
    \item \(f(x, y) \geq 0, \forall (x, y) \in \mathbb{R}\).
    \item \[
          \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}  
          f_{X, Y}(x, y) dx dy = 0
    .\]
    \item For any (measurable) sets \(A, B\),
         \[
             \mathbb{P}(X \in  A, Y \in  B)
             =
            \int_{A} \int_B f_{X, Y}(x, y) dx dy
        .\]
\end{enumerate}

\paragraph{Min and Max Notation}
We may write \(a \vee b = \max(a, b)\) and \(a \wedge b = \min(a, b)\).

\paragraph{Revision: Double Integral Theorems}
\begin{enumerate}
    \item \textbf{Tonelli's Theorem:} Suppose
        \(f: \mathbb{R}^2 \to \mathbb{R}_+\). Then, \[
            \int_\mathbb{R} \int_\mathbb{R} f(x, y) dx dy)
            =
            \int_\mathbb{R} \int_\mathbb{R} f(x, y) dy dx)
        .\]
    \item \textbf{Fubini's Theorem:} Suppose that
        \(f: \mathbb{R}^2 \to \mathbb{R}\). If
        \[
            \int_\mathbb{R} \int_\mathbb{R} \left| f(x, y))\right|dxdy < \infty,
            \text{ or }
            \int_\mathbb{R} \int_\mathbb{R} \left| f(x, y))\right|dydx < \infty,
        \]
        Then, \[
            \int_\mathbb{R} \int_\mathbb{R} f(x, y)) dxdy < \infty,
            =
            \int_\mathbb{R} \int_\mathbb{R} f(x, y)) dydx < \infty,
        .\]
\end{enumerate}

\paragraph{Lemma: Expected Value of Bounded Borel Function}
For any bounded Borel function \(g: \mathbb{R} \to \mathbb{R}\)
and random variables \(X, Y\), if the following sums / integrals
are finite then,  \[
    \mathbb{E}(g(X, Y))
    = \begin{cases}
        \sum_{\forall x} \sum_{\forall y} g(x, y) \mathbb{P}(X = x, Y = y)
        & \text{ discrete }, \\
        \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} 
        g(x, y) f_{X, Y}(x, y) dx dy.
    \end{cases}
.\]

\paragraph{Marginal Probability Density}
This is the name of the individual density functions of a joint
density function.
That is, \(f_X, f_Y\) are the marginal densities of \(f_{X, Y}\).

The marginal densities are given as  \[
    f_X(x) = \int_\mathbb{R} f_{x,y} dy.
.\] Similarly, for the discrete case, \[
    \mathbb{P}(X = x) = \sum_{\forall y} \mathbb{P}(X = x, Y = y).
.\]

\paragraph{Independence}
Two random discrete variables \(X, Y\) are independent if  \[
    \mathbb{P}(X = x, Y = y) = \mathbb{P}(X = x)\mathbb{P}(Y = y),
    \forall x, y
.\]
Similarly, for the continuous case, with a join probability of \(f_{X, Y}\),
 \[
    f_{X, Y} = f_x(x) f_y(y), \forall x, y
.\]

\paragraph{Independent Product of Expectation}
If \(X, Y\) are independent for bounded functions
\(f, g: \mathbb{R} \to \mathbb{R}\) then \[
    \mathbb{E}(g(X)f(Y)) = \mathbb{E}(g(X)) \mathbb{E}(f(Y))
.\]

\paragraph{Conditional Probability}
Conditional probability for functions extends the standard case of sets.
That is, in the discrete case, \[
    \mathbb{P}(X = x | Y = y) =
    \frac{\mathbb{P}(X = x, Y = y)}{\mathbb{P}(Y = y)}.
.\] Similarly for the continuous case, \[
    f_{x | y} \coloneqq
    \frac{f_{x, y}(x, y)}{f_y(y)}
.\]

\paragraph{Multivariate Gaussian}
A random vector \(X = (X_1, X_2)\) is said to be Gaussian with
 \(\mu_X = (\mu x_1, \mu x_2)\) and covariance matrix \(V\) if
  \[
     f_X(x) = \frac{1}{\sqrt{(2\pi)^d |V|}}
     \exp \left( -\frac{1}{2} (X-\mu_X)^T V^{-1}(X- \mu_X) \right) 
 .\]
That is \[
    f_X(x)
.\]
 \(d = 2\) (dimensions),  \(V^{-1}\) is the matrix inverse of  \(V\)
 and  \(|V|\) is the determinant of  \(V\).

 \paragraph{Variance Matrix}
 The variance matrix is a symmetric matrix with entries \[
     V_{ij} = \Cov(X_i, X_j), \text{ where } i, j \in 1, \ldots d
 .\]

 If \(X = (X_1, X_2)\) is multivariate Gaussian then  \((X_i)\)
 for  \(i = 1, 2\) must be one-dimensional Gaussian but, the converse is
 not true.
 

\paragraph{Conditional Variance and Expecation}
Given any bound Borel function \(g\), the conditional expecation of  \(g(x)\)
given the set  \(\{Y = y\} \) is \[
    \mathbb{E}(g(x) | Y = y)
    = \begin{cases}
        \sum_{x} g(x) \mathbb{P}(X = x | Y = y) & \text{ discrete, } \\
        \int_{-\infty}^{\infty}  g(x) f_{X | Y} (x | y) dx
            & \text{ continuous. }
    \end{cases}
.\]
Conditional variance follows such that when given \( Y=y \), \[
    \Var(X | Y = y)
    =
    \mathbb{E}(X^2 | Y = y) - ( \mathbb{E}(X | Y = y) )^2
.\]

\paragraph{Independent Conditional Variance and Expecation}
Let \(X, Y\) be independent random variables. Then,  \[
    \mathbb{E}(X | Y = y) = \mathbb{E}(X)
    \text{ and }
    \Var(X | Y = y) = \Var(X)
.\] 

\paragraph{Covariance}
Let \(X, Y\) be random variables. Then,  \[
    \Cov(X, Y) =
    \mathbb{E}((X - \mathbb{E}(X))) (Y - \mathbb{E}(Y))
.\]

\paragraph{Properties of Covariance}
For random variables \(X, Y\),
 \begin{enumerate}
    \item \(\Cov(X, X) = \Var(X)\).
    \item \( \Cov(X, Y) = \mathbb{E}(XY) - \mathbb{E}(X) \mathbb{E}(Y) \).
    \item \( \Cov(X, Y) = 0 \) if  \( X, Y \) are independent. However
        a zero covariance does not imply independence.
    \item The covariance is bilinear such that
        \( \forall a, b c \in\mathbb{R} \), and random variables \( X, Y, Z \),
         \[
            \Cov(aX + bY, Z) = a\Cov(X, Y) + b\Cov(Y, Z)
        .\]
        The same principle holds for \( \Cov(X, aY+ bZ) \).
\end{enumerate}

\paragraph{Correlation}
The correlation may be thought of as a measure of linear independence.
Note that though independence implies variables are uncorrelated,
uncorrelated variables are not neccesarily independent.

The correlation can be thought of as a normalisation of covariance.
That is, \[
    \Cor(X, Y) \coloneqq
    = \frac{\Cov(X, Y)}{\Var(X) \Var(Y)}
.\]

\paragraph{Properties of Correlation}
Let \( X, Y \) be two random variables.
\begin{enumerate}
    \item \( |\Cor(X, Y)| \leq 1 \)
    \item \( \Cor(X, Y) = 1 \) if and only if there exists an
         \( a, b \in \mathbb{R} \) where \( b < 0 \) such that
         \( \mathbb{P}(Y = a + bX) = 1\).
    \item \( \Cor(X, Y) = 1 \) if and only if there exists an
         \( a, b \in \mathbb{R} \) where \( b > 0 \) such that
         \( \mathbb{P}(Y = a + bX) = 1\).
\end{enumerate}

\paragraph{Transformations - Continuous Random Variables}
A real valued function \( h \) over \( A \subset \mathbb{R} \) 
is said to be monotone on \( A \) if \( h \) is strictly increasing
or decreasing over \( A \).

Note that \( h \) is invertible.

\paragraph{Calculating PDF of Monotone Transformations}
Suppose that \( X \) is a random variable with density \( f_x \).
If \( h \) is monotone over \( \{x : f_X(x) > 0\}  \)
then, the probability density of \( Y \coloneqq h(X)\) 
is given by \[
    f_Y(y) = f_X(x) \left| \frac{dy}{dx} \right| 
    =
    f_X \circ h^{-1} (y) \left| \frac{dh^{-1}(y)}{dy}\right| 
.\]

\subsection{Bivariate Transformations}

Suppose that \( X, Y \) are random variables over \( \mathbb{R} \) with  tranforms
\( U, V \) respectively.
Then, the joint probability density is given by \[
    f_{U, V} = f_{X, Y}(x, y) |\det(J)|
.\]
Observe that \( X = U^{-1}, V = Y^{-1}\). Recall from multivariate
calculus that \( J \) is the Jacobian Matrix such that \[
    J = \begin{pmatrix} 
        \dfrac{dx}{du} & \dfrac{dx}{dv}
                \vspace{3mm}\\
        \dfrac{dy}{du} & \dfrac{dy}{dv}
    \end{pmatrix} 
.\]

\subsection{Sums of Variables}

\paragraph{Convolution Formula}
Suppose \( X, Y \) are independent, continuous r.v. with density  \( f_X \),
\( f_Y \). Let \( Z = X + Y \). Then,  \[
    f_Z(z) = \int_{-\infty}{\infty} f_X(Z - Y) f_Y(y) dy
.\]

In the discrete case, \[
    \mathbb{P}(X + Y = z) = \sum_{y} \mathbb{P}(X = z - y) \mathbb{P}(Y = y)
.\]

\paragraph{Convolutions and Exponential / Gamma}
The method for convolutions can be use to show that if
\( X \sim \Gamma(\alpha , 1) \) and, \( Y \sim \Gamma(\beta, 1) \),
where \( X, Y \) are independent, then, \[
    Z \coloneqq X + Y \sim \Gamma(\a + \beta, 1).
.\]
Observe \( 0 < y < z < \infty \).

\paragraph{Convolutions and Moment Generating Functions}
Suppose that \( X, Y \) are random variables with moment
generating functions. Then, \[
    M_{X + Y}(t) = M_X(t) M_Y(t)
.\]

Generally, if \( \{X_i\}_{i=1}^n  \) is an independent sequence of random variables
then  \[
    M_{\sum_{i=1}^n} x_i (T) = \prod_{i=1}^n M_{X_i} (t)
.\]

